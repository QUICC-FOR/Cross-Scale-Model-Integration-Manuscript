\section*{Discussion}


\subsection*{Comparison with other methods}
The methods provided here expand upon the motivation of hybrid models to develop a more robust and unifying approach for ecological models in a predictive context while striving to overcome some of the limitations characteristic of other integrated approaches. 
Attempting to incorporate underlying ecological processes so that SDMs are not oversimplified by combining different model parameters or scales can be challenging using these hybrid approaches. 
In particular, it can often be difficult to identify parameters that can be used to connect different modeling frameworks and what parameters can produce meaningful responses \citep{Thuiller2013}. 
The methods presented in this study aim to overcome some of these limitations through the use of Bayesian statistics. 
Bayesian methods produce results as an entire distribution instead of a single point, allowing for a comprehensive understanding of the uncertainty of estimates \citep{Link2010}. 
The estimation of model parameters (and predictions) as distributions can also help decrease overparameterization that can be a concern for hybrid models. 
Bayesian frameworks also provide a natural framework for the incorporation of prior information. 
The inability or difficulty to include information from experimental studies or ecological processes at lower scales can be a possible drawback of hybrid models \citep{Thuiller2008, Smolik2010}. 
Even when a link can be formed to include this information, it often is oversimplified to simplify computation \citep{Gallien2010}. 
Finally, Bayesian methods inherently allow for feedbacks or interactions between sub models, which may portray a more realistic response to many environmental dynamics where factors may simultaneously influence the change of one another.

Our approach is an original application of Bayesian multilevel modeling, and can be considered a logical extension of other Bayesian approaches developed to deal with processes that occur at multiple scales while using several models simultaneously. 
In particular, this development has certain similarities with Bayesian model averaging, Bayesian calibration of process-based models and hierarchical Bayesian modeling. 
Bayesian model averaging aims to combine several alternative models to obtain better predictions while taking into account parameter uncertainties \citep{Hoeting1999}, and has been applied numerous times in ecology where the mechanisms underlying a complex phenomenon are often unknown \citep[e.g.][]{Wintle2003, Link2006}. 
Bayesian model averaging considers models operating at the same scale and the posterior distribution is usually determined with Gibbs sampling. 
Bayesian calibration of process-based model focuses on uncertainty of the parameter values in the process-based models, in this case the values of the parameters are calibrated by the model output \citep{VanOijen2005, VanOijen2011, Hartig2012}. 
In contrast with Bayesian model averaging and calibration techniques, our approach handles data and models operating at different hierarchical scales, and uses process-based models to constrain the shape of a generally more correlative metamodel.
Similar hierarchical approaches have been applied to diverse fields, including engineering \citep{Booth2013}, hydraulic conductivity models \citep{Dostert2009, Efendiev2005, Efendiev2005a}, plant physiology \citep{Ogle2008, Ogle2009}, and climate and atmospheric modeling \citep{Zimmerman2005, Mcmillan2010, Kang2012}.
\wt{Why not being more precise and exploratory here. You may actually run a model averaging around your approach, may not you? Indeed, if we think to Ex 1 you could have several native metamodel posterior distributions, right? Or even a direct model averaging from them. As said, you could actually run something like biomod2 to get the distribution of values for each pixel given the models/algorithms and data and use them directly as a posterior into the integration. . 
The integration could also be done in the Ex 2 as far as I could see it. 
It might nicely link your approach to the on-going move of SDM people towards ensemble modeling and forecasting. It also lonk to Bayesian model averaging. 
If I was meant, I would say that a simple use of binomial GLM is a 10-years way back for most people where more 
}

\fd{FD will work on this part: \\
It would be good to compare this approach with the kind of one develop by Moorcroft et al. 2001 : Moorcroft, P. R., Hurtt, G. C., \& Pacala, S. W. (2001). A method for scaling vegetation dynamics: the ecosystem demography model (ED). Ecological monographs, 71(4), 557-586.\\
%Medvigy, D., Wofsy, S. C., Munger, J. W., Hollinger, D. Y., \& Moorcroft, P. R. (2009). Mechanistic scaling of ecosystem function and dynamics in space and time: Ecosystem Demography model version 2. Journal of Geophysical Research: Biogeosciences (2005–2012), 114(G1).\\
Fisher, R., McDowell, N., Purves, D., Moorcroft, P., Sitch, S., Cox, P., \& Ian Woodward, F. (2010). Assessing uncertainties in a second-generation dynamic vegetation model caused by ecological scale limitations. New Phytologist, 187(3), 666-681. 
\\
These models are multilevel and use scaling functions using statisitical moments series.
}

\subsection*{Advantages of Model Integration}
\aa{Discussion: the section ‘Advantages of model integration’, which I wrote, and ‘Applicability of the method’ have several points in common. Maybe they could be merged into a single section: ‘Applicability and advantages of model integration’. Maybe, it should be discussed more extensively how to face the situation in which predictions of correlative model and sub-models are so different that the integrated model performs poorer than the two (I mean, does that mean that the approach is not valid in those cases?)}
\fd{I see that paragraph coming closer to the end of the paper in the accesibility section, maybe}
Models are important tools that are increasingly used by land managers to face challenges associated to decision-making \citep{Guisan2013}
A growing number of models are available, but they can provide diverging answers to very similar questions due to the differences in their assumptions and methodology. 
This can create confusion and even some mistrust towards models, and as a consequence some managers may be discouraged to incorporate model results in their management plans. 
By integrating different types of models---and their outputs---into a common framework, we believe that our approach can contribute to overcome the gap between modelers and practitioners and thus promote wider use of models to support decision-making.
\ia{This is repetitive with the applicability section. I suggest that you stick here on the modeling advantage of your approach and keep the applied advantage for the other section}

Model uncertainty is another key factor affecting applicability of model outputs \citep{Addison2013}.
One of the main strengths of our approach is that it allows for a clear, transparent identification of uncertainties and how they are transmitted throughout the modeling process. 
Transparency in uncertainty can be considered as a sort of sensitivity analysis, in which areas with large uncertainty can be detected and new experimental research or additional data collection can be designed (e.g., Example 1, Figs. \ref{fig:ex1_sampling}, \ref{fig:ex1_precip}).
The new knowledge resulting from this research can be readily incorporated into the metamodel, allowing for an iterative learning process that will undoubtedly contribute to reduce uncertainty in predictions for a wide range of environmental conditions. 
Moreover, our framework embraces change as a fundamental process and is able to adapt and respond to it. 
The ease of incorporating new knowledge to the modeling framework (including new theory or the result of management and experimental efforts), will allow for a rapid adjustment of the predictions and the incorporation of the most recent available knowledge into management plans \citep{Keith2010}.
In an era of continuous and unprecedented change, adaptive approaches such as the one presented here are often highlighted as a pressing need in order to develop strategies to promote ecosystems that are both feasible and resilient \citep[Fig. \ref{fig:management};][]{Seastedt2008, Mori2013}.
\wt{
Perhaps we could also be more precise here. Give examples. Since you are working with forest data, an easy step forward would also be to calibrate growth curve of most tree species in function of both temp and precipitation, even a competition model (see Kunstler et al. 2011 J. of Ecology), that can then be integrated into SDMs. 
Similarly you may also discuss about the NEON and LTER long term sites that could certainly give similar information than the one used in Ex 1. I think we should strenghen the different possibilities to make sure users find the approach useful. It remains to vague to me at this point. 
If you think about the Cheaib et al. paper, you may also suggest that your approach could be run over all different model outputs to integrate thema all together. 
In a way that should re-inforce the link between modelers and long term experimental ecologists that did not really discuss so far since it was difficult to integrate the two informations. 
}


\subsection*{Challenges} 
Although our approach is highly flexible and can be applied in a number of situations, there are some challenges to successfully using the framework.
As with any modeling effort, good model specification with strong links to theory are essential \citep{Austin2007}.
Poorly specified models will produce outputs that are uninformative or misleading, and model integration is not a cure for these problems.
In the worst case, constraining a metamodel with a poor sub-model can result in outcomes that are worse in terms of bias and uncertainty than those produced by a naive model.
We expect model selection will play an important role in applications of our framework, and such schemes can be incorporated relatively easily \citep{Madigan1995, Wasserman2000, Tenan2014}.

In a similar fashion, the quality and availability of data impose a significant constraint on the number and type of models that can be implemented.
The capacity to implement a model is very low if data requirements are prohibitive. 
Adequate coverage of explanatory variables is a significant obstacle, and exploratory analyses can be a significant aid in understanding how data coverage impacts resulting predictions \citep{Mckenney2002}.
Integration can ameliorate these issues to some extent by using supplemental information (and conceptual advances) in additional sub-models where coverage is weak (e.g., Example 1, Figs. \ref{fig:ex1_precip}, \ref{fig:ex1_map}).
For example, \citet{Freckleton2009} estimated that data are too scant to successfully develop highly mechanistic models predicting weed population numbers. 
A strong asset of our approach is that it can be used without the full suite of data that would be required to run a fully mechanistic model. 
Given that the metamodel is correlative, it can be effectively implemented with, e.g., only presence-absence data, or, in the case where true absences are difficult to obtain, with presences and pseudo absences (provided sufficient care is used in interpreting the results of such a model).
Then any additional mechanistic data that are available will enhance predictions by constraining outputs of the metamodel. 

Finally, determining the functions to use to express the likelihood of the sub-models given the metamodel (i.e., Eqs. \ref{eq:ex1_integrated}, \ref{eq:integrated2}) is a critical point.
In the context of integrated models, the challenge is three fold: (i) which spatial and temporal scales (i.e. which processes), are to be considered, (ii) how to build a scaling function that is consistent with the metamodel and (iii) how error and uncertainty are propagated from the scales of the sub-models scale to that of the metamodel. 
Although we argue that our proposed framework is able to easily deal with very different scales and that the Bayesian framework allows for an efficient integration of uncertainty throughout all scales considered, the building of scaling function deserves further investigation. 
We have shown in our examples that even simple scaling functions can provide reasonable constraints on the metamodel. 
However, with the objective to take into account all know processes and models, it is likely that the modeling process may include multiple scaling functions, each at different scales. 
Indeed, if species distributions are a function of, e.g. poopulation growth rate \citep{Sykes1996, Guisan2000}, they will involve processes at the individual (e.g. competition) or cellular (e.g. photosynthesis) scales. 
Such very large differences in spatial scales imply that simple functions will be inadequate, requiring more traditional upscaling methods. 
Therefore, intermediate models and processes will need to be introduced, and this will require additional parameters, assumptions, and data, and will add uncertainty to the final integrated model.



\subsection*{Applicability of the method}
\defcitealias{TERN2013}{TERN, 2013} %% this is needed to make the organizational citation in the following paragraph work correctly
Integrated approaches have gained momentum in recent years, with integrative science being featured as a central theme for several science-based governmental organizations around the world \citep[e.g.][\citetalias{TERN2013}]{Bernier2013}. 
Incorporating information from multiple sources, particularly with respect to uncertainty,  fosters a connection between scientifically-generated knowledge and policy, and is therefore an important tool for adaptive management \citep[][Fig. \ref{fig:management}; ]{Rehme2011}.
Such approaches are needed in formulating management plans for vulnerable species and ecosystems to avoid basing decisions on too-narrow subsets of the available information \citep{Dawson2011}.
The flexibility of our approach may also represent an advantage for management by easing the integration of specific decision making criteria (e.g. desired grain and extent of outputs) into model development. 
Successful use of an integrated modeling approach will always remain dependent on an intimate understanding of the decision-making process by modelers, emphasizing the importance of close collaboration between with practitioners at all stages of model development \citep{Guisan2013}.

The transparency of our approach is also a clear advantage. 
To be useful, models should be transparent analytical tools, not black boxes \citep{Addison2013}. 
A key criterion for model applicability is the explicit and detailed communication of specific model objectives, characteristics, limits, and uncertainties, as well as its ecological foundation \citep{Guisan2013}.
Integrating sub-models allows for clear specification of desired model outputs (via the specification of the metamodel) while easily retaining important ecological objectives (via careful specification of sub-models).
Insufficiently documented models are difficult to assess in real world situation, and therefore only have limited relevance for decision support \citep{Addison2013, Guisan2013}.
Model workflow documentation becomes more crucial in the case of integrated modeling approaches, which incorporates information from various scales and resolutions within a number of aggregated sub-models. 
A transparent and well documented workflow describing the process of model integration ensures reproducibility and applicability (Fig. \ref{fig:management}). 

\subsection*{Accessibility}
\mjf{Accessibility of what? Modeling? Data?}
\mt{maybe remove the label and relocate to challenges}
Results of models predicting species ranges are increasingly used by resource managers, conservation biologists, or forest ecologists for formulating or adjusting recommendations. 
The utility of such models depends on their ability to help evaluate events beyond the bounds of the available data, including in future situations constrained by climate change. 
In this context, practitioners may judge model usefulness on two criteria: (1) Does the model provide reliable predictions at the needed time and space scales, and (2) can the model be implemented given the available data and technical expertise? 
Since most decision makers work at local space scales and follow specific time frames, modelers face the obvious challenge of providing detailed outputs while preserving implementation capacity.
In terms of model outputs, our framework is transparent in terms of uncertainty by virtue of providing easily interpretable probability distributions for model parameters and predictions.
However, developing the models requires careful model specification, understanding of applied Bayesian methods, and, in some cases, extensive programming.
In many cases, off-the-shelf software \citep[e.g.,][]{R, RJAGS} can adequately express the model likelihoods with minimal programming, but more complicated models will require the development of custom software.
Thus, this framework will require significant investment in developing customized code for the samplers in order to actually estimate parameters, which may be technically challenging for some practitioners and may limit the adoption of these methods by both practitioners and decision-makers.
However, the same was true of all statistical techniques or modeling approaches when they were initially developed. 
In addition, some of the computation costs that were associated with many techniques have now vanished, and even the conceptual challenges associated Bayesian statistics are being reduced as they gain recognition in the scientific literature. 
We therefore argue that our proposed approach as a strong potential for direct use in our real world where climate is quickly changing and conservation practices must be adjusted accordingly.
We further argue that forging stronger collaborations between modelers, decision-makers, and practitioners will improve the incorporation of this and other new methods into applied ecology and conservation.

\subsection*{Future directions}
Hierarchical Bayesian modeling requires data at every scale to fit the parameters required to constrain one level with another.
Moreover, beyond the needs for additional data, more knowledge of the important processes driving species range limits is needed to allow for extrapolation beyond the scope of the original datasets to address future changes due to climate and land-use changes.
This knowledge can be obtained through a series of experimental manipulations and by substituting space for time data along environmental gradients.
When experiments are possible, gaps in knowledge can be inferred using surrogate variables, theoretical models and spatially explicit models to explore the range of outcomes possible \citep{Fortin2012}. 
Another way to gain knowledge is to simplify species complexity into a series of states and to use state-transition models (e.g., Markov chain, semi-Markov chain, matrix projection model, integral projection model) which require more theoretical or coarser knowledge of species responses to biotic and abiotic changes.
Although fewer states allow us to better capture the essence of species responses to environmental conditions, knowledge is still required to model feedback effects across processes and spatio-temporal scales.
Hence we need to determine which processes are the most important at a given scale and how to weight their effects while scaling-up.
Our meta-modeling approach can act as a springboard for incorporating imperfect knowledge of how multi-scale processes influence species ranges.
By incorporating this knowledge into species distribution models, we can hopefully both reduce bias and more accurately assess model uncertainty when forecasting future changes to species ranges.